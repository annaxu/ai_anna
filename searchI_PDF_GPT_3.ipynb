{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf_loader import PDFLoader\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import gradio as gr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter,MarkdownTextSplitter\n",
    "#from langchain.document_loaders import UnstructuredFileLoader,UnstructuredMarkdownLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "#from langchain.document_loaders import UnstructuredImageLoader\n",
    "from rapidocr_onnxruntime import RapidOCR\n",
    "\n",
    "max_token = 8000\n",
    "split_doc_size = 1000\n",
    "chunk_overlap = 50\n",
    "pdf_file = 'data/IRM_Help.pdf'\n",
    "work_dir = '/Users/I069899/Documents/study/AI/ai_anna'\n",
    "\n",
    "\n",
    "\n",
    "#分割pdf文件\n",
    "def load_pdf_splitter(pdf_file, chunk_size=max_token, chunk_overlap=chunk_overlap):\n",
    "    docs = load_pdf_file(pdf_file)\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    #默认展示分割后第一段内容\n",
    "    print('split_docs[0]: ', split_docs[0])\n",
    "    return split_docs\n",
    "\n",
    "#加载pdf文件\n",
    "def load_pdf_file(pdf_file):    \n",
    "    loader = UnstructuredPDFLoader(os.path.join(work_dir, pdf_file))\n",
    "    docs = loader.load()\n",
    "    print('pdf:\\n',docs[0].page_content[:split_doc_size])\n",
    "    return docs\n",
    "\n",
    "def load_pdf_by_pdfloader(pdf_file):\n",
    "    # 读取PDF文件并将其内容转换为文本。\n",
    "    pdf_loader = PDFLoader(file_path=pdf_file)\n",
    "    data = pdf_loader.load()\n",
    "    return data\n",
    "\n",
    "#try load_pdf_splitter\n",
    "split_docs = load_pdf_splitter(pdf_file, max_token, chunk_overlap)\n",
    "print(\" pdf data load by UnstructuredPDFLoader is \", split_docs)\n",
    "docs = load_pdf_by_pdfloader(pdf_file)\n",
    "print(\" pdf data load by PDFLoader is \", docs)\n",
    "\n",
    "\n",
    "# 将文本切分为小块，确保每块都不超过token的最大长度限制。\n",
    "# 这里假设你有一个名为tokenize的函数可以进行这个操作\n",
    "tokens = tokenize(data.get_text(), max_length=max_token)\n",
    "\n",
    "# 使用embedding模型将文本块转换为向量，并将这些向量存储到向量数据库中。\n",
    "embedder = AzureOpenAIEmbeddings()\n",
    "db = FAISS.from_documents(tokens, embedder)\n",
    "\n",
    "# 将数据库保存到本地\n",
    "db_path = 'your_db_path'\n",
    "db.save_local(db_path)\n",
    "\n",
    "# 从本地加载数据库\n",
    "new_db = FAISS.load_local(db_path, embedder)\n",
    "\n",
    "# 使用Gradio开发用户界面，允许用户输入查询并执行语义搜索。\n",
    "\n",
    "def search_reviews(product_description, n=3):\n",
    "    product_embedding = embedder.embed(product_description)\n",
    "    similarities = [cosine_similarity([vec], [product_embedding])[0][0] for vec in new_db.vectors]\n",
    "    df = pd.DataFrame({'tokens': tokens, 'similarity': similarities})\n",
    "\n",
    "    results = (\n",
    "        df.sort_values(\"similarity\", ascending=False)\n",
    "        .head(n)\n",
    "        .tokens.str.replace(\"Title: \", \"\")\n",
    "        .str.replace(\"; Content:\", \": \")\n",
    "    )\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "iface = gr.Interface(fn=search_reviews, inputs=\"text\", outputs=\"text\")\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
