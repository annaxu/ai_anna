{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sessionid is  3501E82DEEE1A8BC6C1C316D660575AF\n",
      "********* begin print first_paragraph ************************** \n",
      "********* first_paragraph is  In today's rapidly changing world, knowledge is constantly being refreshed, and it's essential to adapt and evolve in order to avoid becoming obsolete. The impact of macro-economic factors on our company and industry necessitates a proactive approach skill development and strategic influence to ensure that our team remains at the forefront of our organization. By leveraging our collective skills and expertise, we have the opportunity to consolidate efforts across multiple teams and achieve larger, more impactful goals. While individuals may excel in their own right, it's through collaborative efforts that we can truly propel our path forward.\n",
      "********* end print first_paragraph ************************** \n",
      "encoded is  [644, 3432, 596, 19019, 10223, 1917, 11, 6677, 374, 15320, 1694, 66229, 11, 323, 433, 596, 7718, 311, 10737, 323, 38680, 304, 2015, 311, 5766, 10671, 47166, 13, 578, 5536, 315, 18563, 77249, 9547, 389, 1057, 2883, 323, 5064, 4541, 82829, 264, 59314, 5603, 10151, 4500, 323, 19092, 10383, 311, 6106, 430, 1057, 2128, 8625, 520, 279, 52301, 315, 1057, 7471, 13, 3296, 77582, 1057, 22498, 7512, 323, 19248, 11, 584, 617, 279, 6776, 311, 74421, 9045, 4028, 5361, 7411, 323, 11322, 8294, 11, 810, 98990, 9021, 13, 6104, 7931, 1253, 25555, 304, 872, 1866, 1314, 11, 433, 596, 1555, 40806, 9045, 430, 584, 649, 9615, 89483, 1057, 1853, 4741, 13]\n",
      "number of tokens is  111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "# from tiktoken import Tokenizer\n",
    "# from tiktoken.models import TokenCounts\n",
    "import tiktoken\n",
    "\n",
    "#tokenizer = Tokenizer()\n",
    "model_name = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "enc4GPT4 = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "# #encoding = tiktoken.get_encoding(encoding_name)\n",
    "# encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "env_path = os.getenv(\"HOME\") + \"/Documents/src/openai/.env\"\n",
    "load_dotenv(dotenv_path=env_path, verbose=True)\n",
    "sessionid= os.getenv(\"JSESSIONID\")\n",
    "print(\" sessionid is \", sessionid)\n",
    "\n",
    "def get_num_tokens_from_string(payloadString: str, model_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoded = model_name.encode(payloadString)\n",
    "    print(\"encoded is \" , encoded )\n",
    "    num_tokens = len(encoded)\n",
    "    return num_tokens\n",
    "\n",
    "def get_first_paragraph(url, headers, cookies):\n",
    "    # Fetch the page content\n",
    "    response = requests.get(url, headers=headers, cookies=cookies)\n",
    "    response.raise_for_status()  # Raise an exception in case of failure\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract the first paragraph\n",
    "    first_paragraph = soup.find('p').get_text()\n",
    "    first_paragraph = first_paragraph.rstrip()\n",
    "    \n",
    "\n",
    "    # Print the first paragraph\n",
    "    print(\"********* begin print first_paragraph ************************** \")\n",
    "    print(\"********* first_paragraph is \", first_paragraph)\n",
    "    print(\"********* end print first_paragraph ************************** \")\n",
    "\n",
    "    # Tokenize the text and count the tokens\n",
    "    #tokens = list(tokenizer.tokenize(first_paragraph))\n",
    "    #print(\"***** the tokens are: *******\", tokens)\n",
    "    #return len(tokens)\n",
    "    return first_paragraph\n",
    "\n",
    "headers = {\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,/;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"none\",\n",
    "    \"Sec-Fetch-User\": \"?1\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "}\n",
    "\n",
    "cookies = {\n",
    "    \"JSESSIONID\": os.getenv(\"JSESSIONID\"),\n",
    "}\n",
    "\n",
    "url = 'https://wiki.one.int.sap/wiki/display/eurekaplus/Portfolio+Management'\n",
    "first_paragraph = get_first_paragraph(url, headers, cookies)\n",
    "\n",
    "number_of_tokens = get_num_tokens_from_string(first_paragraph, model_name)\n",
    "\n",
    "print(\"number of tokens is \" , number_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
