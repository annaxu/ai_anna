{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf:\n",
      " 2/26/2024\n",
      "\n",
      "Administrator's Guide\n",
      "\n",
      "Generated on: 2024-02-26 02:30:17 GMT+0000\n",
      "\n",
      "SAP Intelligent Returns Management | 1.0\n",
      "\n",
      "PUBLIC\n",
      "\n",
      "Original content: https://help.sap.com/docs/returns/fb041e275d964ef0944a80f2592d411a?locale=en-\n",
      "\n",
      "US&state=PRODUCTION&version=1.0\n",
      "\n",
      "Warning\n",
      "\n",
      "This document has been generated from the SAP Help Portal and is an incomplete version of the oﬃcial SAP product\n",
      "\n",
      "documentation. The information included in custom documentation may not re\u0000ect the arrangement of topics in the SAP Help\n",
      "\n",
      "Portal, and may be missing important aspects and/or correlations to other topics. For this reason, it is not for productive use.\n",
      "\n",
      "For more information, please visit the https://help.sap.com/docs/disclaimer.\n",
      "\n",
      "This is custom documentation. For more information, please visit the SAP Help Portal\n",
      "\n",
      "1\n",
      "\n",
      "2/26/2024\n",
      "\n",
      "Global Settings\n",
      "\n",
      "This section describes the basic settings about number, date and time format, company pro\u0000le and so on.\n",
      "\n",
      "For more information, see the following topics:\n",
      "\n",
      "Con\u0000guring Regiona\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "first argument must be string or compiled pattern",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 70\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#try load_pdf_splitter\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#split_docs = load_pdf_splitter(pdf_file, max_token, chunk_overlap)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#split_docs = load_pdf_splitter('/Users/I069899/Documents/study/AI/ai_anna/data/IRM_Help.pdf', split_doc_size, chunk_overlap)\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m split_docs \u001b[38;5;241m=\u001b[39m load_pdf_splitter(split_doc_size, chunk_overlap)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pdf data load by UnstructuredPDFLoader is \u001b[39m\u001b[38;5;124m\"\u001b[39m, split_docs)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# docs = load_pdf_by_pdfloader(pdf_file)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# print(\" pdf data load by PDFLoader is \", docs)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 49\u001b[0m, in \u001b[0;36mload_pdf_splitter\u001b[0;34m(chunk_size, chunk_overlap)\u001b[0m\n\u001b[1;32m     47\u001b[0m docs \u001b[38;5;241m=\u001b[39m load_pdf_file()\n\u001b[1;32m     48\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m CharacterTextSplitter(chunk_size, chunk_overlap)\n\u001b[0;32m---> 49\u001b[0m split_docs \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(docs)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#默认展示分割后第一段内容\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_docs[0]: \u001b[39m\u001b[38;5;124m'\u001b[39m, split_docs[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.11/site-packages/langchain_text_splitters/base.py:95\u001b[0m, in \u001b[0;36mTextSplitter.split_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     93\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mpage_content)\n\u001b[1;32m     94\u001b[0m     metadatas\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_documents(texts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.11/site-packages/langchain_text_splitters/base.py:78\u001b[0m, in \u001b[0;36mTextSplitter.create_documents\u001b[0;34m(self, texts, metadatas)\u001b[0m\n\u001b[1;32m     76\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     77\u001b[0m previous_chunk_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_text(text):\n\u001b[1;32m     79\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(_metadatas[i])\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_start_index:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.11/site-packages/langchain_text_splitters/character.py:26\u001b[0m, in \u001b[0;36mCharacterTextSplitter.split_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# First we naively split the large input into a bunch of smaller ones.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m separator \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separator \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_separator_regex \u001b[38;5;28;01melse\u001b[39;00m re\u001b[38;5;241m.\u001b[39mescape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separator)\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m splits \u001b[38;5;241m=\u001b[39m _split_text_with_regex(text, separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keep_separator)\n\u001b[1;32m     27\u001b[0m _separator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keep_separator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separator\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_splits(splits, _separator)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.11/site-packages/langchain_text_splitters/character.py:44\u001b[0m, in \u001b[0;36m_split_text_with_regex\u001b[0;34m(text, separator, keep_separator)\u001b[0m\n\u001b[1;32m     42\u001b[0m         splits \u001b[38;5;241m=\u001b[39m [_splits[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m+\u001b[39m splits\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         splits \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(separator, text)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.11/re/__init__.py:206\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(pattern, string, maxsplit, flags)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(pattern, string, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Split the source string by the occurrences of the pattern,\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    returning a list containing the resulting substrings.  If\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    capturing parentheses are used in pattern, then the text of all\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    and the remainder of the string is returned as the final element\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    of the list.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msplit(string, maxsplit)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.11/re/__init__.py:286\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pattern\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _compiler\u001b[38;5;241m.\u001b[39misstring(pattern):\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m&\u001b[39m T:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: first argument must be string or compiled pattern"
     ]
    }
   ],
   "source": [
    "#from langchain.document_loaders.pdf_loader import PDFLoader\n",
    "#langchain_community.document_loaders.pdf.UnstructuredPDFLoader\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import gradio as gr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "#from langchain.document_loaders import UnstructuredFileLoader,UnstructuredMarkdownLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "#from langchain.document_loaders import UnstructuredImageLoader\n",
    "#from rapidocr_onnxruntime import RapidOCR\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import pdfminer\n",
    "from pdfminer import utils\n",
    "\n",
    "max_token = 8000\n",
    "split_doc_size = 1000\n",
    "chunk_overlap = 50\n",
    "pdf_file_name = 'data/IRM_Help.pdf'\n",
    "work_dir = '/Users/I069899/Documents/study/AI/ai_anna/'\n",
    "db_path = \"data/vectordb\"\n",
    "\n",
    "env_path = os.getenv(\"HOME\") + \"/Documents/src/openai/.env\"\n",
    "load_dotenv(dotenv_path=env_path, verbose=True)\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://pvg-azure-openai-uk-south.openai.azure.com\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2023-05-15\"\n",
    ")\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "#分割pdf文件\n",
    "def load_pdf_splitter(chunk_size, chunk_overlap):\n",
    "    docs = load_pdf_file()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size, chunk_overlap)\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    #默认展示分割后第一段内容\n",
    "    print('split_docs[0]: ', split_docs[0])\n",
    "    return split_docs\n",
    "\n",
    "#加载pdf文件\n",
    "def load_pdf_file(): \n",
    "    # if not isinstance(pdf_file, str):\n",
    "    #   raise TypeError(\"pdf_file must be a string\")\n",
    "    # if not os.path.exists(pdf_file):\n",
    "    #   raise FileNotFoundError(f\"The file {pdf_file} does not exist\")\n",
    "   \n",
    "    #loader = UnstructuredPDFLoader(os.path.join(work_dir, pdf_file))\n",
    "    loader = UnstructuredPDFLoader('/Users/I069899/Documents/study/AI/ai_anna/data/IRM_Help.pdf')\n",
    "    docs = loader.load()\n",
    "    print('pdf:\\n',docs[0].page_content[:split_doc_size])\n",
    "    return docs\n",
    "\n",
    "#try load_pdf_splitter\n",
    "#split_docs = load_pdf_splitter(pdf_file, max_token, chunk_overlap)\n",
    "#split_docs = load_pdf_splitter('/Users/I069899/Documents/study/AI/ai_anna/data/IRM_Help.pdf', split_doc_size, chunk_overlap)\n",
    "split_docs = load_pdf_splitter(split_doc_size, chunk_overlap)\n",
    "print(\" pdf data load by UnstructuredPDFLoader is \", split_docs)\n",
    "# docs = load_pdf_by_pdfloader(pdf_file)\n",
    "# print(\" pdf data load by PDFLoader is \", docs)\n",
    "print(\"**** load pdf complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
